\documentclass{article}
\title{Devoir 3: Revue de la Littérature}
\author
{
    Guillermo Martinez (matricule x)
    \and
    Dereck Piché (matricule 20177385)
    \and
    Jonas Gabirot (matricule x)
}


\begin{document}
\maketitle

\section*{Aptamères}
\subsection*{Description des aptamères}
*insert*
\subsection*{Motivation de l'étude des aptamères}
*insert*

\section*{Algorithmes d'apprentissage automatique}
\subsection*{Multilayer Perceptrons}
*insert*
\subsection*{Reccurent Neural Networks}
*insert*
\subsection*{Transformers}
Selon nos hypothèses, l'architecture *transformer*  est de loin
la plus appropriée pour notre tâche. les transformers utilisent 
un mecanisme d'attention multi-tête et l'auto-attention. Soit
un séquence de tokens d'entrée. Alors le mécanisme d'attention 
créera pour chaque token d'entrée un autre token qui est une 
combinaison linéaire des autres vecteurs d'entrée donnés. Ainsi, 
on \cite{transPaper}. Comme on suppose une forte corrélation 
entre les éléments de l'entrée, (...) Positional encodings ()
\subsection*{G-flow nets}
*insert*

\section*{Prédiction avec Algorithmes classiques}
\subsection*{Algorithmes classiques utilisés}
*Brève description et infos sur les performances*
\subsection*{Algorithmes classiques utilisés}
*insert*

\bibliography{bibliography}
\bibliographystyle{plain}
\end{document}