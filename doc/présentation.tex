\documentclass{beamer}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amscd}
\usepackage{blkarray} % Required for inserting images
\usetheme{metropolis}
\metroset{block=fill} 

\title{Modélisation d'Aptamères d'ADN IFT 3710}
\author{Dereck Piché, Jonas Gabirot, Guillermo Martinez}
\institute{Université de Montréal}
\date{Avril 2023}


\begin{document}

\frame{\titlepage}

\section{Contexte}
\begin{frame}{ADN-Aptamères}
\begin{block}{Propriétés fondamentales}
\begin{enumerate}
    \item Phénotype: affinité, se plie avec une fonction définie
    \item Génotype (séquence): réplicable IN VITRO (SELEX)
\end{enumerate}
\end{block}

\begin{block}{Biosenseur}
\begin{enumerate}
    \item Structure chimique simple: 20-100 bases (insertion)
    \item Liaison: changement conformationnel (signal)
    \item Stabilité: supérieure aux ARN-aptamères
\end{enumerate}
\end{block}

\begin{block}{Nanotechnologie et Computation}
\begin{enumerate}
    \item Stockage d'information: 2x supérieure au stockage binaire, pendant des milliers d'années
    \item Qualité de stockage: dépend de la stabilité de la séquence.
\end{enumerate}
\end{block}

\end{frame}

\begin{frame}{Minimum free energy (MFE) structure}

\begin{block}{Structure secondaire de l'ADN}
\begin{enumerate}
    \item Deux séquences (1D) qui se lient en une hélice (2D)
    \item Stabilité: du pairage entre les bases des deux séquences, et donc de la taille et de l'arrangement des séquences
\end{enumerate}
\end{block}

\begin{figure} \label{fig:ADN_structure}
    \caption{Structures de l'ADN} \center
    \includegraphics[width=0.35\textwidth]{images/DNA_structure.png}
\end{figure}

Méthode NUPACK: estime MFE de la structure secondaire en fonction d'une analyse thermodynamique, Nearest-neighbor paramètres empiriques (Zadeh et al. 2010).

\end{frame}

\begin{frame}{Objectif}
L'estimation du MFE d'une séquence d'ADN se complexifie en fonction de sa longeur. Cependant, les méthodes thermodynamiques comme celle de NUPACK sont rapides et relativement précises (r=70-80)

Objectif de recherche: entraîner des modèles d'apprentissage machine à prédire le MFE des structures secondaire de séquences d'ADN de 30 bases.

Retombées: L'apprentissage de représentations pour la fonction du MFE (étant donné des entrées séquentielles) peuvent réduire l'espace de recherche de réseaux génératifs, tels que les GFlowNets, pour générer des séquences d'ADN stables en minimisant la fonction apprise du MFE.

\end{frame}

\begin{frame}{Visualisation données}

\begin{figure} \label{fig:MFE_nupack}
    \caption{Distribution des MFEs dans notre dataset} \center
    \includegraphics[width=0.35\textwidth]{images/MFE_nupack.png}
\end{figure}

\end{frame}

\section{Baseline: MLP}
\begin{frame}{MLP}

Comme baseline, nous avons choisi un réseau MLP standard. Les MLPs ne font aucune supposition sur la structure de la séquence, ce qui permet de comparer avec d'autres modèles qui se basent sur des hypothèses différentes. Ils sont aussi faciles à implémenter et entraîner. 
\begin{equation*}
    \begin{CD}
        @. Seq \in R^{120}
        @>Lin. (\times 20)>> 
        @>ReLU (\times 20)>> 
        @>Lin.>> 
        Energie \in R
    \end{CD}
\end{equation*}

\begin{block}{Hyperparamètres}
\begin{enumerate}
    \item Taux d'apprentissage: 0.0003
    \item Nombre de paramètres: 1 005 241
\end{enumerate}
\end{block}

\end{frame}


\begin{frame}{Résultats du MLP}
\begin{figure} \label{fig:mlp}
    \caption{Précision du MLP selon MSE} \center
    \includegraphics[width=0.7\textwidth]{images/mlp.png}
\end{figure}
\end{frame}


\section{Long Short Term Memory (LSTM)}

\begin{frame}{LSTM}
Le modèle LSTM (Long Short-Term Memory) est une variante des réseaux de neurones récurrents (RNN) qui permet de mieux gérer les problèmes liés à la mémoire à long terme. Il utilise des portes, qui sont des fonctions non linéaires, pour contrôler l'information qui entre ou sort de la mémoire.
\end{frame}


\begin{frame}{Portes}

Il y a trois types de portes : la porte d'oubli (forget gate) permet au modèle de décider ce qu'il faut oublier de la mémoire à long terme, la porte d'entrée (input gate) permet d'ajouter de nouvelles informations à la mémoire à long terme et la porte de sortie (output gate) permet de déterminer quelle information doit être renvoyée en sortie.
\begin{figure} \label{fig:architecture_lstm}
    \caption{Architecture LSTM} \center
    \includegraphics[width=0.6\textwidth]{images/lstm-schema.png}
\end{figure}

\end{frame}

\begin{frame}{Architecture du LSTM}

\begin{equation*}
    \begin{CD}
        @. Seq \in R^{120}
        @>LSTM1>> 
        @>LSTM2>> 
        @>Lin.>> 
        Energie \in R
    \end{CD}
\end{equation*}

\begin{block}{Hyperparamètres}
\begin{enumerate}
    \item Taux d'apprentissage: 0.0003
    \item Nombre de paramètres: 1 008 991 
    \item Dropout: 0.6
\end{enumerate}
\end{block}

\end{frame}

\begin{frame}{Résultats du LSTM}

\begin{figure} \label{fig:lstm}
    \caption{Précision du LSTM selon MSE} \center
    \includegraphics[width=0.7\textwidth]{images/lstm.png}
\end{figure}

\end{frame}
\section{Transformeurs}

\begin{frame}{Attention}
Un système d'attention retourne une somme des vecteurs \emph{Valeurs} selon 
une mesure de similarité entre les vecteurs \emph{Requêtes} et les vecteurs \emph{Clés}.
Cette mesure de similarité est appelée l'attention.
    \begin{block}{Système d'attention}
        $\sum_{j} a(q, k_j) v_j$
    \end{block}
Les transformeurs utilisent un système d'auto-attention, qui vise à produire
une séquence transformée dans laquelle chaque jeton est codé en effectuant une somme pondérée des
autres jetons présents dans sa séquence (l'incluant).
    \begin{block}{Jeton codé}
        $t_{i}^{'} = \sum_{j} a(W^{q}t_{i}, W^{k}t_j) W^{v}t_j$
    \end{block}
\end{frame}

\begin{frame}{Attention}
Dans les transformeurs, on utilise une attention spéciale :
\begin{block}{Attention au produit scalaire mis à l'échelle}
    \[ a(q, k) = softmax( \frac{q^Tk}{\sqrt{d}}) \]
\end{block}


{\bf Pourquoi le produit scalaire?}
On sait déjà que le produit scalaire peut être vu comme une mesure de la similarité entre
les deux vecteurs. Cela permet beaucoup de liberté au transformeur, qui obtient les vecteurs clés et les vecteurs requêtes en fonction des vecteurs jetons avant d'effectuer le produit scalaire. On le laisse donc créer sa propre mesure de distance, en quelque sorte.

\end{frame}

\begin{frame}{}
    
{\bf Pourquoi le softmax?}
Pour que les coéfficients somment à 1. (*expliquer l'intuition)

{\bf Pourquoi la division par la racine de la taille? }
Pour éviter des erreurs numériques. On épargne les détails.

\end{frame}

\begin{frame}{Couche de codage}
    Une tête d'attention retourne donc une séquence de la même taille que son entrée (sauf exceptions).
    Une couche de codage contient plusieurs têtes d'attention! Les sorties de ces 
    têtes d'attention sont concaténées dans {\it un gros vecteur} et on applique une transformation 
    linéaire (avec activation ReLU pour obtenir de l'expressivité non linéaire) à ce vecteur pour obtenir une sortie 
    de dimension arbitraire.
\end{frame}

\begin{frame}{Architecture du transformeur}
\begin{figure} \label{fig:transformer}
    \caption{Transformer de traduction dans AIAYN} \center
    \includegraphics[width=0.5\textwidth]{images/2023-04-04-14-37-09.png}
\end{figure}
\end{frame}

\begin{frame}{Architecture du transformeur}
\begin{equation*}
    \begin{CD}
        @. Seq \in R^{30}
        @>Emb.>> 
        @>Cod. (\times 3)>> 
        @>Lin.>> Energie \in R
    \end{CD}
\end{equation*}

\begin{block}{Hyperparamètres}
\begin{enumerate}
    \item Taux d'apprentissage: $10^{-4}$
    \item Dimension d'embedding: 16
    \item Nombre de têtes: 8
    \item Nombre de paramètres: 1 786 753 
    \item Codage positionnel: sincos (AIAYN)
\end{enumerate}
\end{block}
\end{frame}


\begin{frame}{Résultats du transformeur}
\begin{figure} \label{fig:transformer_accuracy}
    \caption{Précision du transformeur selon MSE} \center
    \includegraphics[width=0.7\textwidth]{images/2023-04-02-13-48-49.png}
\end{figure}
\end{frame}

\begin{frame}{Comparaison des résultats}
\begin{figure}
    \caption{MSE des 3 modèles} \center
    \includegraphics[width=0.7\textwidth]{images/comparaison.png}
\end{figure}
\end{frame}

\section{Conclusion}

\begin{frame}{Contraste MFE expérimental/simulation}

Technique d'estimation thermodynamique du MFE similaire à celle de NUPACK: r=75 avec MFE expérimentale.
\begin{figure} \label{fig:MFE_exp_simul}
    \caption{Distribution des MFEs dans notre dataset} \center
    \includegraphics[width=0.35\textwidth]{images/MFE_exp_simul.png}
\end{figure}

\end{frame}

\begin{frame}{Suggestions d'étapes à suivre}

1) Tester la validité prédictive du Transformer sur des séquences ADN dont le MFE a été établi empiriquement.

2) Augmenter la taille du dataset à 10-100 million de séquences pour inclure des plus fortes minimas en termes de MFE. Puis, tester la validité prédictive du transformer sur ces minimas. Re-entraîner au besoin.

3) Entraîner des GFlowNets à générer des séquences d'ADN qui minimisent le MFE, pour aller au-delà de l'aléatoire.

4) Compléter le protocole End-to-End-DNA (SELEX) pour transformer nos séquences d'ADN les plus stables en aptamères avec des affinités spécifiques.

\end{frame}

\begin{frame}

\begin{figure} \label{fig:fractal}
    \center
    \includegraphics[width=1\textwidth]{images/Fractal.jpg}
\end{figure}

\end{frame}

\end{document}
