\documentclass{article}
\usepackage{amsmath}
\usepackage{graphicx}
\title{Results}

\author
{
    Dereck Pich√© \and
    Guillermo Martinez \and
    Jonas Gabirot \and
}

\begin{document}
\maketitle

\section{Results}


\subsection{Baseline: MLP}

\begin{figure} \label{fig:template}
    \caption{Voici le template} \center
    \includegraphics[width=0.7\textwidth]{images/2023-03-28-10-29-38.png}
\end{figure}

\subsection{LSTM}

\subsection{Regression Transformer}
We built a Transfomer for our regression task. The transformer contains encoder layers. Each of the encoder layers contains 8 attention heads with a ReLU activated linear layer. We trained the transformer on 9000000 DNA sequences of length 30 and tested it's MSE loss on the remaining 100000 training examples. The transformer used the same positionnal encoding as the one proposed in \cite{transformers}. The DNA sequences came in string form. Each of the 4 tokens were transformed into an integer and fed into a learned embedding layer that transformed them into vectors of 16 dimensions. The number is a bit arbitrary, but since it works really well we left it there. The accuracy troughout the epochs can be seen in figure \ref{fig:transformer_accuracy}

\begin{figure} \label{fig:transformer_accuracy}
    \caption{Voici le template} \center
    \includegraphics[width=0.7\textwidth]{images/2023-04-02-13-48-49.png}
\end{figure}

\section{Analysis}


\subsection{Comparisons}

\section{Conclusion}

\bibliography{bibliography}
\bibliographystyle{plain}
\end{document}
