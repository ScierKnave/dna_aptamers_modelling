\documentclass{article}
\usepackage{amsmath}
\usepackage{graphicx}
\title{Results}

\author
{
    Dereck Pich√© \and
    Guillermo Martinez \and
    Jonas Gabirot \and
}

\begin{document}
\maketitle

\section{Results}

\subsection{Baseline: XGB}
We trained and tested an Extreme Gradient Boosting (XGB) regressor using DNA sequences as features and their respective free-energy levels as labels. 900 000 samples were used for training and 100 000 samples for testing. The model's train and test performance were assessed by calculating the Mean Square Error (MSE) between the predicted and actual free energy levels given specific DNA sequences. 0.993166 and 1.016363 MSE were achieved on train and test evaluation respectively. These results will be used as baselines to evaluate further models' predictive performance.


\subsection{Baseline: MLP}
We built a standard MLP model as a deep learning baseline for our regression task. It consists of 21 fully connected layers. We trained the model on $900, 000$ DNA sequences of length $30$ and tested it's MSE loss on the remaining $100,000$ training examples. The DNA sequences were preprocessed into one hot encoded vectors of length $120$. The accuracy troughout the epochs can be seen in figure \ref{fig:mlp} 
\begin{figure} \label{fig:mlp}
    \caption{MLP loss} \center
    \includegraphics[width=0.7\textwidth]{images/mlp.png}
\end{figure}

\subsection{LSTM}
We built a model using the regressive LSTM architecture for our regression task. Our regression LSTM is made up of a 2 LSTM layers and one fully connected layer. We trained the model on $900, 000$ DNA sequences of length $30$ and tested it's MSE loss on the remaining $100,000$ training examples. The DNA sequences were preprocessed into one hot encoded vectors of length $120$. The accuracy troughout the epochs can be seen in figure \ref{fig:lstm}
\begin{figure} \label{fig:lstm}
    \caption{LSTM loss} \center
    \includegraphics[width=0.7\textwidth]{images/lstm.png}
\end{figure}

\subsection{Regression Transformer}
We built a model using the main components of the Transfomer architecture proposed in \cite{transformers} for our regression task. Our regression transformer is made up of a sequence of encoder layers. Each of the encoder layers contains 8 attention heads with a ReLU activated linear layer. We trained the transformer on $900, 000$ DNA sequences of length $30$ and tested it's MSE loss on the remaining $100,000$ training examples. The transformer used the same positionnal encoding as the one proposed in \cite{transformers}. The DNA sequences came in string form. Each of the 4 tokens were transformed into an integer and fed into a learned embedding layer that transformed them into vectors of 16 dimensions. The number of dimensions of the embeddings is a bit arbitrary, but since it works really well we left it there. The accuracy troughout the epochs can be seen in figure \ref{fig:transformer_accuracy}. 

\begin{figure} \label{fig:transformer_accuracy}
    \caption{Transformer loss} \center
    \includegraphics[width=0.7\textwidth]{images/2023-04-02-13-48-49.png}
\end{figure}

\section{Analysis}


\subsection{Comparisons}
\begin{figure} \label{fig:comparison}
    \caption{Loss comparison} \center
    \includegraphics[width=0.7\textwidth]{images/compare.png}
\end{figure}
\section{Conclusion}

\bibliography{bibliography}
\bibliographystyle{plain}
\end{document}
