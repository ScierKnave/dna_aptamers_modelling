{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only 3 encoder layers\n",
    "# With sin cos encoding\n",
    "# batch_size: 300\n",
    "\n",
    "# Transformer HyperHPeters\n",
    "# The length of our token sequences\n",
    "ntokens_HP = 30\n",
    "# Number of attention heads\n",
    "nheads_HP = 8\n",
    "# The encoding dimensions of our tokens\n",
    "token_encode_size_HP = 4\n",
    "# The embedding string size\n",
    "# Pytorch will cut this embedded sequence \n",
    "# and give an equal amount to each head, different \n",
    "# than in theory \n",
    "embed_size_HP = token_encode_size_HP *  nheads_HP\n",
    "# Output size of the heads, which \n",
    "# learn an embedding.\n",
    "head_embedsize_HP = 4\n",
    "# For some reason PyTorch needs us to do this manually\n",
    "d_model_HP = head_embedsize_HP * nheads_HP\n",
    "# No dropout for now\n",
    "dropout_HP = 0\n",
    "# Number of encoding layers\n",
    "n_encoders_HP = 3\n",
    "\n",
    "# Standard stuff\n",
    "activation_HP = \"relu\"\n",
    "layer_norm_eps_HP = 1e-5\n",
    "batch_first_HP = True\n",
    "norm_first_HP = False\n",
    "\n",
    "# Trainig and validation Hyperparameters\n",
    "datasetsize_HP = 1_000_000\n",
    "split_HP = 0.9\n",
    "batchsize_HP = 350\n",
    "batchsize_HP = min(batchsize_HP, (int)(datasetsize_HP*0.2))\n",
    "nepochs_HP = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_HP = nn.MSELoss()\n",
    "learnrate_HP = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only one encoder layer!\n",
    "# With sin cos encoding\n",
    "# batch_size: 32\n",
    "\n",
    "[1] Training loss: 1.180\n",
    "[1] Test loss: 1.103\n",
    "[2] Training loss: 1.076\n",
    "[2] Test loss: 1.110\n",
    "[3] Training loss: 1.060\n",
    "[3] Test loss: 1.076\n",
    "[4] Training loss: 1.053\n",
    "[4] Test loss: 1.039\n",
    "[5] Training loss: 1.048\n",
    "[5] Test loss: 1.031\n",
    "[6] Training loss: 1.046\n",
    "[6] Test loss: 1.034\n",
    "[7] Training loss: 1.043\n",
    "[7] Test loss: 1.056\n",
    "[8] Training loss: 1.041\n",
    "[8] Test loss: 1.031\n",
    "[9] Training loss: 1.039\n",
    "[9] Test loss: 1.039\n",
    "[10] Training loss: 1.038\n",
    "[10] Test loss: 1.048\n",
    "[11] Training loss: 1.037\n",
    "[11] Test loss: 1.041\n",
    "[12] Training loss: 1.036\n",
    "[12] Test loss: 1.022\n",
    "[13] Training loss: 1.035\n",
    "[13] Test loss: 1.060\n",
    "[14] Training loss: 1.034\n",
    "[14] Test loss: 1.029\n",
    "[15] Training loss: 1.034\n",
    "[15] Test loss: 1.030\n",
    "[16] Training loss: 1.033\n",
    "[16] Test loss: 1.038\n",
    "[17] Training loss: 1.032\n",
    "[17] Test loss: 1.036\n",
    "[18] Training loss: 1.032\n",
    "[18] Test loss: 1.033\n",
    "[19] Training loss: 1.031\n",
    "[19] Test loss: 1.028\n",
    "[20] Training loss: 1.031\n",
    "[20] Test loss: 1.022\n",
    "[21] Training loss: 1.030\n",
    "[21] Test loss: 1.027\n",
    "[22] Training loss: 1.030\n",
    "[22] Test loss: 1.021\n",
    "[23] Training loss: 1.030\n",
    "[23] Test loss: 1.026\n",
    "[24] Training loss: 1.029\n",
    "[24] Test loss: 1.029"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
